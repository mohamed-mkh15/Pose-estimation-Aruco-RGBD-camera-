{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPamltx5yfPKyAW4LGR34fZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamed-mkh15/Pose-estimation-Aruco-RGBD-camera-/blob/main/aruco_pose_estimation_RGBDcam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Aruco marker position and orientation"
      ],
      "metadata": {
        "id": "-XSM1d_in9Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## All imports\n",
        "import pyrealsense2 as rs  # python lib to read data from the realsense camera\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rc('font',family='Times New Roman')\n",
        "import matplotlib.image as mpimg\n",
        "import glob\n",
        "from IPython.display import clear_output\n",
        "\n",
        "from scipy.spatial.transform import Rotation"
      ],
      "metadata": {
        "id": "a0QEv-MBn-dH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camera calibratioCameran Method #1: saving images and using cv2 calibration methods"
      ],
      "metadata": {
        "id": "Zyv5wv32oZ7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "# 1- Save some color images for calibration\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Configure depth and color streams\n",
        "pipeline = rs.pipeline()\n",
        "config = rs.config()\n",
        "\n",
        "# Get device product line for setting a supporting resolution\n",
        "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
        "pipeline_profile = config.resolve(pipeline_wrapper)\n",
        "device = pipeline_profile.get_device()\n",
        "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
        "\n",
        "found_rgb = False\n",
        "for s in device.sensors:\n",
        "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
        "        found_rgb = True\n",
        "        break\n",
        "if not found_rgb:\n",
        "    print(\"The demo requires Depth camera with Color sensor\")\n",
        "    exit(0)\n",
        "\n",
        "# Set the resolution for our D455 RGBD camera\n",
        "resolution_width = 1280 # pixels\n",
        "resolution_height = 720 # pixels\n",
        "frame_rate = 30\n",
        "\n",
        "# enable the stream\n",
        "config.enable_stream(rs.stream.depth, resolution_width, resolution_height, rs.format.z16, frame_rate)\n",
        "config.enable_stream(rs.stream.color, resolution_width, resolution_height, rs.format.bgr8, frame_rate)\n",
        "\n",
        "# Start streaming\n",
        "pipeline.start(config)\n",
        "\n",
        "i=0\n",
        "try:\n",
        "    while True:\n",
        "\n",
        "        # Wait for a coherent pair of frames: depth and color\n",
        "        frames = pipeline.wait_for_frames()\n",
        "        align = rs.align(rs.stream.color)\n",
        "        aligned_frames = align.process(frames)\n",
        "        color_frame = aligned_frames.first(rs.stream.color)\n",
        "\n",
        "        if not color_frame:\n",
        "            continue\n",
        "\n",
        "        # Convert images to numpy arrays\n",
        "        color_image = np.asanyarray(color_frame.get_data())\n",
        "\n",
        "        # save images in a folder\n",
        "        cv2.imwrite('calib_images/calib.%05d.jpg' % (i),color_image)\n",
        "        i+=1\n",
        "\n",
        "finally:\n",
        "\n",
        "    # Stop streaming\n",
        "    pipeline.stop()"
      ],
      "metadata": {
        "id": "42e9PlL5oE-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2- Import a chess board image and get it's corners using cv2"
      ],
      "metadata": {
        "id": "RSm_Pg9qoe7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare object points\n",
        "nx = 13 #13 # number of inside corners in x\n",
        "ny = 8 #8 # number of inside corners in y\n",
        "\n",
        "# Make a list of calibration images\n",
        "images = glob.glob('calib_images/calib*.jpg')\n",
        "# Select any index to grab an image from the list\n",
        "idx = 1\n",
        "# Read in the image\n",
        "img = mpimg.imread(images[idx])\n",
        "\n",
        "# Convert to grayscale\n",
        "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Find the chessboard corners\n",
        "ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
        "cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
        "plt.imshow(img)"
      ],
      "metadata": {
        "id": "EVPvj42GoFRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3- Apply interensic Calibration"
      ],
      "metadata": {
        "id": "5RTNDJPhoih-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
        "objp = np.zeros((ny * nx,3), np.float32)\n",
        "objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
        "\n",
        "# Arrays to store object points and image points from all the images.\n",
        "objpoints = [] # 3d points in real world space\n",
        "imgpoints = [] # 2d points in image plane.\n",
        "\n",
        "# Step through the list and search for chessboard corners\n",
        "for idx in range(len(images)):\n",
        "    img = mpimg.imread(images[idx])\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "    # Find the chessboard corners\n",
        "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
        "\n",
        "    # If found, add object points, image points\n",
        "    #if corners is not None:\n",
        "    True_count = 0\n",
        "    if ret:\n",
        "#         objp = np.zeros((len(corners),3), np.float32)\n",
        "#         print('--',idx , len(objp), len(corners))\n",
        "        objpoints.append(objp)#objp\n",
        "        imgpoints.append(corners)\n",
        "        True_count+=1\n",
        "print('True_count = ', True_count)\n",
        "\n",
        "img_size = (img.shape[1], img.shape[0])\n",
        "\n",
        "# Do camera calibration given object points and image points\n",
        "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)"
      ],
      "metadata": {
        "id": "9DBljloNoiCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import a chess board image and get it's corners using cv2"
      ],
      "metadata": {
        "id": "aIhQdf-OooHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Test one image\n",
        "images = glob.glob('calib_images/calib*.jpg')\n",
        "idx = 14\n",
        "test_img = mpimg.imread(images[idx])\n",
        "test_img_size = (test_img.shape[1], test_img.shape[0])\n",
        "\n",
        "# Perform undistortion\n",
        "undist = cv2.undistort(test_img, mtx, dist, None, mtx)\n",
        "\n",
        "# Visualize undistortion\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 9))\n",
        "f.tight_layout()\n",
        "ax1.imshow(test_img)\n",
        "ax1.set_title('Original Image', fontsize=12)\n",
        "ax2.imshow(undist)\n",
        "ax2.set_title('Undistorted Image', fontsize=12)\n",
        "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)"
      ],
      "metadata": {
        "id": "MuvkN12holMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Camira calibration Method #2: using libersense own methods"
      ],
      "metadata": {
        "id": "qD1LC4qJoxli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "# Read and view RealSense data and save internsic matrix\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "# Configure depth and color streams\n",
        "pipeline = rs.pipeline()\n",
        "config = rs.config()\n",
        "\n",
        "# Get device product line for setting a supporting resolution\n",
        "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
        "pipeline_profile = config.resolve(pipeline_wrapper)\n",
        "device = pipeline_profile.get_device()\n",
        "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
        "\n",
        "found_rgb = False\n",
        "for s in device.sensors:\n",
        "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
        "        found_rgb = True\n",
        "        break\n",
        "if not found_rgb:\n",
        "    print(\"The demo requires Depth camera with Color sensor\")\n",
        "    exit(0)\n",
        "\n",
        "# Set the resolution for our D455 RGBD camera\n",
        "resolution_width = 1280 # pixels\n",
        "resolution_height = 720 # pixels\n",
        "frame_rate = 30\n",
        "\n",
        "# Set the chessboard parameters for calibration\n",
        "chessboard_width = 9 # squares\n",
        "chessboard_height = 14 # squares\n",
        "square_size = 0.04 # meters\n",
        "chessboard_params = [chessboard_height, chessboard_width, square_size]\n",
        "\n",
        "# enable the stream\n",
        "config.enable_stream(rs.stream.depth, resolution_width, resolution_height, rs.format.z16, frame_rate)\n",
        "config.enable_stream(rs.stream.color, resolution_width, resolution_height, rs.format.bgr8, frame_rate)\n",
        "\n",
        "\n",
        "# Start streaming\n",
        "cfg = pipeline.start(config)\n",
        "\n",
        "# calculate intrinsic matrix\n",
        "profile = cfg.get_stream(rs.stream.color)  # Fetch stream profile for color stream\n",
        "intr = profile.as_video_stream_profile().get_intrinsics()\n",
        "print(\"intr: \", intr)\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        # Wait for a coherent pair of frames: depth and color\n",
        "        frames = pipeline.wait_for_frames()\n",
        "\n",
        "        align = rs.align(rs.stream.color)\n",
        "        aligned_frames = align.process(frames)\n",
        "        color_frame = aligned_frames.first(rs.stream.color)\n",
        "        depth_frame = aligned_frames.get_depth_frame() # aligned depth frame\n",
        "\n",
        "        #depth_frame = frames.get_depth_frame()\n",
        "        #color_frame = frames.get_color_frame()\n",
        "        if not depth_frame or not color_frame:\n",
        "            continue\n",
        "\n",
        "        # Convert images to numpy arrays\n",
        "        depth_image = np.asanyarray(depth_frame.get_data())\n",
        "        color_image = np.asanyarray(color_frame.get_data())\n",
        "\n",
        "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
        "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
        "\n",
        "        depth_colormap_dim = depth_colormap.shape\n",
        "        color_colormap_dim = color_image.shape\n",
        "\n",
        "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
        "        if depth_colormap_dim != color_colormap_dim:\n",
        "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
        "            images = np.hstack((color_image, depth_colormap))\n",
        "        else:\n",
        "            images = np.hstack((color_image, depth_colormap))\n",
        "\n",
        "        # Show images\n",
        "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
        "        cv2.imshow('RealSense', images)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "finally:\n",
        "\n",
        "    # Stop streaming\n",
        "    pipeline.stop()"
      ],
      "metadata": {
        "id": "-FeKL1oroy-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Test one image\n",
        "images = glob.glob('calib_images/calib*.jpg')\n",
        "idx = 14\n",
        "test_img = mpimg.imread(images[idx])\n",
        "test_img_size = (test_img.shape[1], test_img.shape[0])\n",
        "\n",
        "# Perform undistortion\n",
        "mtx = np.array([[intr.fx, 0.     , intr.ppx],\n",
        "                [0.    , intr.fy, intr.ppy],\n",
        "                [0.    , 0.     , 1.     ]])\n",
        "\n",
        "dist = np.array(intr.coeffs)\n",
        "undist = cv2.undistort(test_img, mtx, dist, None, mtx)\n",
        "\n",
        "# Visualize undistortion\n",
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 9))\n",
        "f.tight_layout()\n",
        "ax1.imshow(test_img)\n",
        "ax1.set_title('Original Image', fontsize=12)\n",
        "ax2.imshow(undist)\n",
        "ax2.set_title('Undistorted Image', fontsize=12)\n",
        "plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "WQHlpTtYo2az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now apply pose estimations using aruco code"
      ],
      "metadata": {
        "id": "V8vGB6ndo7Kh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define pose estimation function"
      ],
      "metadata": {
        "id": "0zEG7cQco9wV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pose_esitmation(frame, aruco_dict_type, matrix_coefficients, distortion_coefficients, marker_length = 0.049):\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    cv2.aruco_dict = cv2.aruco.Dictionary_get(aruco_dict_type)\n",
        "    parameters = cv2.aruco.DetectorParameters_create()\n",
        "\n",
        "\n",
        "    corners, ids, rejected_img_points = cv2.aruco.detectMarkers(gray, cv2.aruco_dict,parameters=parameters,\n",
        "        cameraMatrix=matrix_coefficients,\n",
        "        distCoeff=distortion_coefficients)\n",
        "\n",
        "    # If markers are detected\n",
        "    found=False\n",
        "    if len(corners) > 0:\n",
        "        found=True\n",
        "        for i in range(0, len(ids)):\n",
        "            # Estimate pose of each marker and return the values rvec and tvec---(different from those\n",
        "            #of camera coefficients)\n",
        "            rvec, tvec, markerPoints = cv2.aruco.estimatePoseSingleMarkers(corners[i], marker_length , matrix_coefficients,\n",
        "                                                                           distortion_coefficients)\n",
        "            # Draw a square around the markers\n",
        "            cv2.aruco.drawDetectedMarkers(frame, corners)\n",
        "\n",
        "            # Draw Axis\n",
        "            cv2.aruco.drawAxis(frame, matrix_coefficients, distortion_coefficients, rvec, tvec, 0.01)\n",
        "\n",
        "            RR = Rotation.from_rotvec(rvec[0])\n",
        "            np.save(\"rvec\", np.array(RR.as_matrix()))\n",
        "            np.save(\"tvec\", tvec.reshape(3,-1))\n",
        "            RR= Rotation.from_rotvec(rvec[0])\n",
        "\n",
        "    if found:\n",
        "        return frame, np.array(RR.as_matrix()), tvec.reshape(3,-1), found\n",
        "    else:\n",
        "        return frame, np.zeros(3),np.zeros(3), found"
      ],
      "metadata": {
        "id": "uYOVu6Ibo7pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "# Validate pose of aruco marker\n",
        "#-------------------------------------------------------------------------------------------------------------------\n",
        "aruco_dict_type = cv2.aruco.DICT_5X5_100\n",
        "\n",
        "# Configure depth and color streams\n",
        "pipeline = rs.pipeline()\n",
        "config = rs.config()\n",
        "\n",
        "# Get device product line for setting a supporting resolution\n",
        "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
        "pipeline_profile = config.resolve(pipeline_wrapper)\n",
        "device = pipeline_profile.get_device()\n",
        "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
        "\n",
        "found_rgb = False\n",
        "for s in device.sensors:\n",
        "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
        "        found_rgb = True\n",
        "        break\n",
        "if not found_rgb:\n",
        "    print(\"The demo requires Depth camera with Color sensor\")\n",
        "    exit(0)\n",
        "\n",
        "# Set the resolution for our D455 RGBD camera\n",
        "resolution_width = 1280 # pixels\n",
        "resolution_height = 720 # pixels\n",
        "frame_rate = 30\n",
        "\n",
        "# enable the stream\n",
        "config.enable_stream(rs.stream.depth, resolution_width, resolution_height, rs.format.z16, frame_rate)\n",
        "config.enable_stream(rs.stream.color, resolution_width, resolution_height, rs.format.bgr8, frame_rate)\n",
        "\n",
        "\n",
        "# Start streaming\n",
        "cfg = pipeline.start(config)\n",
        "\n",
        "# calculate intrinsic matrix\n",
        "profile = cfg.get_stream(rs.stream.color)  # Fetch stream profile for color stream\n",
        "intr = profile.as_video_stream_profile().get_intrinsics()\n",
        "mtx = np.array([[intr.fx, 0.     , intr.ppx],\n",
        "                [0.    , intr.fy, intr.ppy],\n",
        "                [0.    , 0.     , 1.     ]])\n",
        "\n",
        "dist = np.array(intr.coeffs)\n",
        "print(\"intr: \", intr)\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        # Wait for a coherent pair of frames: depth and color\n",
        "        frames = pipeline.wait_for_frames()\n",
        "\n",
        "        align = rs.align(rs.stream.color)\n",
        "        aligned_frames = align.process(frames)\n",
        "        color_frame = aligned_frames.first(rs.stream.color)\n",
        "        depth_frame = aligned_frames.get_depth_frame() # aligned depth frame\n",
        "\n",
        "        #depth_frame = frames.get_depth_frame()\n",
        "        #color_frame = frames.get_color_frame()\n",
        "        if not depth_frame or not color_frame:\n",
        "            continue\n",
        "\n",
        "        # Convert images to numpy arrays\n",
        "        depth_image = np.asanyarray(depth_frame.get_data())\n",
        "        color_image = np.asanyarray(color_frame.get_data())\n",
        "\n",
        "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
        "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
        "\n",
        "        depth_colormap_dim = depth_colormap.shape\n",
        "        color_colormap_dim = color_image.shape\n",
        "\n",
        "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
        "        if depth_colormap_dim != color_colormap_dim:\n",
        "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
        "            images = np.hstack((color_image, depth_colormap))\n",
        "        else:\n",
        "            images = np.hstack((color_image, depth_colormap))\n",
        "\n",
        "\n",
        "        # calculate position\n",
        "        output,rvec_marker,tvec_marker,found = pose_esitmation(color_image, aruco_dict_type, mtx, dist, 0.049)\n",
        "\n",
        "        if found:\n",
        "            cube = Rotation.from_matrix(rvec_marker)\n",
        "            #thetay=np.arccos(rvec_markertool[0,1,2])*180/np.pi\n",
        "            #thetax=np.arccos(rvec_marker[0,0,2])*180/np.pi\n",
        "\n",
        "            T_CB = np.array([[rvec_base[0,0,0],rvec_base[0,0,1],rvec_base[0,0,2],tvec_base[0,0]],[rvec_base[0,1,0],rvec_base[0,1,1],rvec_base[0,1,2],tvec_base[1,0]],  [rvec_base[0,2,0],rvec_base[0,2,1],rvec_base[0,2,2],tvec_base[2,0]],[0,0,0,1]])\n",
        "            T_CM = np.array([[rvec_marker[0,0,0],rvec_marker[0,0,1],rvec_marker[0,0,2],tvec_marker[0,0]],[rvec_marker[0,1,0],rvec_marker[0,1,1],rvec_marker[0,1,2],tvec_marker[1,0]],[rvec_marker[0,2,0],rvec_marker[0,2,1],rvec_marker[0,2,2],tvec_marker[2,0]],[0,0,0,1]])\n",
        "            T_transl = np.array([[1,0,0,0.1553],[0,1,0,0],[0,0,1,-0.011],[0,0,0,1]])\n",
        "    #         T_transl=np.array([[1,0,0,0.425],[0,1,0,-0.4],[0,0,1,-0.0105+0.0295],[0,0,0,1]])\n",
        "            T_BC=np.linalg.inv(T_CB)\n",
        "            T_final=T_transl @ T_BC @ T_CM\n",
        "            aru_x = T_final[0,3]\n",
        "            aru_y = T_final[1,3]\n",
        "            aru_z = T_final[2,3]\n",
        "            #print(\"x=\",T_final[0,3],\"y=\",T_final[1,3],\"z=\",T_final[2,3])\n",
        "            print(\"x=\",aru_x, \"y=\",aru_y,\"z=\",aru_z)\n",
        "        cv2.imshow('Estimated Pose', output)\n",
        "\n",
        "\n",
        "\n",
        "        # Show images\n",
        "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
        "        cv2.imshow('RealSense', images)\n",
        "        cv2.waitKey(1)\n",
        "\n",
        "finally:\n",
        "\n",
        "    # Stop streaming\n",
        "    pipeline.stop()"
      ],
      "metadata": {
        "id": "U4lCfmUnpBJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pqa80u-epFDK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}